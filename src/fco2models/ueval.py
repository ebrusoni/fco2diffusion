import json
import torch
from diffusers import DDPMScheduler

def load_models(models_info):
    models = dict()
    for model_name, info in models_info.items():
        save_dir, model_path, model_class = info
        model, noise_scheduler, params, losses = load_model(save_dir, model_path, model_class)
        models[model_name] = {
            'model': model,
            'noise_scheduler': noise_scheduler,
            'params': params,
            'losses': losses
        }
    return models

import numpy as np
def print_loss_info(model_dict):
    train_losses = np.array(model_dict['losses']['train_losses'])
    val_losses = np.array(model_dict['losses']['val_losses'])

    min_epoch = np.argmin(train_losses)
    print('min epoch:', min_epoch)
    print('min train loss:', train_losses[min_epoch])

    min_los_avg = np.mean(val_losses, axis=1)
    min_epoch_val = np.argmin(min_los_avg)
    print('min val loss epoch:', min_epoch_val)
    print('min val loss:', min_los_avg[min_epoch_val])
    print('----------------------------------')
        
def load_model(save_dir, model_path, model_class):
    """load model used for both diffusion and baseline models"""
    #read model hyperparameters
    with open(save_dir+'hyperparameters.json', 'r') as f:
        params = json.load(f)

    with open(save_dir+'losses.json', 'r') as f:
        losses = json.load(f)
    
    model_params = params['model_params']
    noise_params = params['noise_params']

    # load the model
    model = model_class(**model_params)
    model.load_state_dict(torch.load(save_dir+model_path))
    model.eval()
    if noise_params is None:
        noise_scheduler = None
    else:
        noise_scheduler = DDPMScheduler(**noise_params)

    return model, noise_scheduler, params, losses


def plot_samples(ax, samples, truth, title):
    """Plot samples and truth values generated by the given model."""
    ax.plot(samples.T, color='blue', alpha=0.5)
    ax.plot(samples.mean(axis=0), '^-', label='reconstructed fco2', color='black', linewidth=3)
    # plot devations from the mean
    ax.fill_between(np.arange(0, 64), samples.mean(axis=0) - samples.std(axis=0),
                     samples.mean(axis=0) + samples.std(axis=0), color='blue', alpha=0.2)
    ax.plot(truth.flatten(), '-o', label='true fco2', color='red', linewidth=3)
    ax.set_title(title)
    ax.set_xlabel('distance')
    ax.set_ylabel('fco2')
    ax.grid()
    ax.legend()

def rec_sample(sample_context, model, noise_scheduler, timesteps):
    """
    Reconstruct  fco2 samples from <timesteps> noise corruption. Use for sanity check.
    """
    sample = torch.from_numpy(sample_context[0:1, :]).float()
    context = torch.from_numpy(sample_context[1:, :]).float()
    # add nan mask to the context
    nan_mask = ~torch.isnan(sample)
    context = torch.cat([context, nan_mask.float()], dim=0)

    nan_sample = torch.where(nan_mask, sample, torch.zeros_like(sample))
    # sample 100 times and plot the distribution of reconstructed fco2
    reconstructed_fco2s = []
    noisy_samples = []
    model.to('cpu')
    for _ in range(100):
        noise = torch.randn_like(nan_sample)
        noisy_sample = noise_scheduler.add_noise(nan_sample, noise, timesteps)
        input = torch.cat([noisy_sample, context], dim=0).unsqueeze(0)
        with torch.no_grad():
            output = model(input, timesteps)
    
        nan_fco2 = noise_scheduler.step(output[0], timesteps.item(), noisy_sample).pred_original_sample
        rec_fco2 = torch.where(nan_mask, nan_fco2, np.nan) # put in nans for better plotting
        reconstructed_fco2s.append(rec_fco2.numpy().flatten())
        # save noisy sample for plotting
        nan_noisy_sample = torch.where(nan_mask, noisy_sample, np.nan)
        noisy_samples.append(nan_noisy_sample.numpy().flatten())
    
    reconstructed_fco2s = np.array(reconstructed_fco2s)
    noisy_samples = np.array(noisy_samples)

    return reconstructed_fco2s, noisy_samples

def rescale(ds, stats, mode):
    """
    Rescale the dataset using the given stats.
    ds shape: (n_samples, features, n_bins)
    """
    train_means = stats['train_means']
    train_stds = stats['train_stds']
    train_mins = stats['train_mins']
    train_maxs = stats['train_maxs']
    for i in range(ds.shape[1]):
        if mode == 'mean_std':
            ds[:, i] = ds[:, i] * train_stds[i] + train_means[i]
        elif mode == 'min_max':
            ds[:, i] = (ds[:, i] + 1) * (train_maxs[i] - train_mins[i]) / 2 + train_mins[i]
        else:
            raise ValueError(f"Unknown mode: {mode}. Use 'mean_std' or 'min_max'.")
    return ds 


from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error
from scipy.stats import pearsonr
def get_error_stats(samples, truth):
    """
    Calculate the error statistics between the samples and the truth values.
    samples: (n_samples, n_rec, n_bins)
    truth: (n_samples, 1, n_bins)
    """

    means = np.nanmean(samples, axis=1)[:, np.newaxis, :]
    
    nan_mask = np.isnan(truth)
    rmse = root_mean_squared_error(truth[~nan_mask], means[~nan_mask])
    mae = mean_absolute_error(truth[~nan_mask], means[~nan_mask])
    r2 = r2_score(truth[~nan_mask], means[~nan_mask])
    #bias
    bias = np.nanmean(truth - means, axis=2)

    corrs = np.zeros(truth.shape[0])
    for i in range(truth.shape[0]):
        nan_mask = np.isnan(truth[i])
        corr, _ = pearsonr(truth[i][~nan_mask], means[i][~nan_mask])
        corrs[i] = corr
    
    mean_corr = np.nanmean(corrs)
    mean_corr_std = np.nanstd(corrs)

    print("Error statistics:")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R2: {r2:.4f}")
    print(f"Bias: {bias.mean():.4f} ± {bias.std():.4f}")
    print(f"Mean correlation: {mean_corr:.4f} ± {mean_corr_std:.4f}")

    return dict({
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'bias': bias.mean(),
        'bias_std': bias.std(),
        'mean_corr': mean_corr,
        'mean_corr_std': mean_corr_std
    })

