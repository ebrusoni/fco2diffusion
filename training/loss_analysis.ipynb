{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60481eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fco2models.utraining import prep_data\n",
    "val_df = pd.read_parquet('../data/training_data/valdf_100km_random_reshaped.pq')\n",
    "vald_df2021 = pd.read_parquet('../data/training_data/df_100km_random_reshaped_2021.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db41b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:51:54,837 - INFO - Filling missing sss_cci values with salt_soda values\n",
      "2025-04-15 15:51:54,911 - INFO - predictors: ['sst_cci', 'sss_cci', 'chl_globcolour']\n",
      "2025-04-15 15:51:55,038 - INFO - clipping fco2 values to 0-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after filtering:  22441\n",
      "(3, 22441, 64) (22441, 64)\n",
      "(3, 22441, 64) (1, 22441, 64)\n",
      "number of fco2 measurements greater than 500:  7189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:51:55,369 - INFO - Filling missing sss_cci values with salt_soda values\n",
      "2025-04-15 15:51:55,390 - INFO - predictors: ['sst_cci', 'sss_cci', 'chl_globcolour']\n",
      "2025-04-15 15:51:55,440 - INFO - clipping fco2 values to 0-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after filtering:  11327\n",
      "(3, 11327, 64) (11327, 64)\n",
      "(3, 11327, 64) (1, 11327, 64)\n",
      "number of fco2 measurements greater than 500:  17121\n",
      "val_ds shape:  (33768, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "predictors = ['sst_cci', 'sss_cci', 'chl_globcolour']\n",
    "val_ds = prep_data(val_df, predictors)\n",
    "vald_ds2021 = prep_data(vald_df2021, predictors)\n",
    "val_ds = np.concatenate((val_ds, vald_ds2021), axis=0)\n",
    "print(\"val_ds shape: \", val_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7ef6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33768, 4, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99448f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DDPMScheduler, UNet1DModel\n",
    "from fco2models.models import MLP, UNet2DModelWrapper\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def load_model(save_dir, model_path, model_class):\n",
    "    #read model hyperparameters\n",
    "    with open(save_dir+'hyperparameters.json', 'r') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    with open(save_dir+'losses.json', 'r') as f:\n",
    "        losses = json.load(f)\n",
    "    \n",
    "    model_params = params['model_params']\n",
    "    noise_params = params['noise_params']\n",
    "\n",
    "    # load the model\n",
    "    model = model_class(**model_params)\n",
    "    model.load_state_dict(torch.load(save_dir+model_path))\n",
    "    model.eval()\n",
    "    noise_scheduler = DDPMScheduler(**noise_params)\n",
    "\n",
    "    return model, noise_scheduler, params, losses\n",
    "\n",
    "# load model\n",
    "save_path = '../models/renko/unet2d_noattn/'\n",
    "model_path = 'e_100.pt'\n",
    "model_class = UNet2DModelWrapper\n",
    "\n",
    "\n",
    "model, noise_scheduler, params, losses = load_model(save_path, model_path, model_class)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabdea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch with minimum training loss:  99\n",
      "Minimum training loss:  0.024658476514286366\n",
      "Epoch with minimum validation loss:  98\n",
      "Minimum validation loss:  0.0780834779717569\n"
     ]
    }
   ],
   "source": [
    "train_losses = losses['train_losses']\n",
    "val_losses = losses['val_losses']\n",
    "# print epoch with minimum loss\n",
    "print(\"Epoch with minimum training loss: \", np.argmin(train_losses))\n",
    "print(\"Minimum training loss: \", np.min(train_losses))\n",
    "\n",
    "val_losses_mean = np.mean(val_losses, axis=1)\n",
    "print(\"Epoch with minimum validation loss: \", np.argmin(val_losses_mean))\n",
    "print(\"Minimum validation loss: \", np.min(val_losses_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce88013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_ds shape:  torch.Size([5000, 3, 64])\n",
      "Denoising samples\n",
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/40 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# denoise the samples\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDenoising samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43mfull_denoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjump\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\OceanCO2_data-20250225T145156Z-001\\dataset-exploration-main\\src\\fco2models\\utraining.py:175\u001b[0m, in \u001b[0;36mfull_denoise\u001b[1;34m(model, noise_scheduler, context_loader, jump)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# context = context.unsqueeze(0)\u001b[39;00m\n\u001b[0;32m    174\u001b[0m sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 175\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    176\u001b[0m sample_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m, context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    177\u001b[0m sample_context[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m=\u001b[39m sample\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from fco2models.utraining import full_denoise\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "n_rec = 10\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(val_ds)\n",
    "train_maxs = params['train_maxs']\n",
    "train_mins = params['train_mins']\n",
    "for i in range(val_ds.shape[1]):\n",
    "    val_ds[:, i, :] = 2 * (val_ds[:, i, :] - train_mins[i]) / (train_maxs[i] - train_mins[i]) - 1\n",
    "context = val_ds[:500, 1:, :]\n",
    "context_ds = torch.from_numpy(np.repeat(context, n_rec, axis=0)).float()\n",
    "print(\"context_ds shape: \", context_ds.shape)\n",
    "context_loader = DataLoader(context_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # denoise the samples\n",
    "    print(\"Denoising samples\")\n",
    "    samples = full_denoise(model, noise_scheduler, context_loader, jump=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9eb32b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1, 64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fda4288",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43msamples\u001b[49m[:\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mT, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample 1 - 10\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_ds[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mT, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDenoised samples vs original\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(samples[:10, 1, :].T, label='sample 1 - 10', color='blue', alpha=0.5)\n",
    "plt.plot(val_ds[1, 0, :].T, label='original', color='red', alpha=1, linewidth=2)\n",
    "plt.title('Denoised samples vs original')\n",
    "plt.xlabel('bins')\n",
    "plt.ylabel('fCO2')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773fd0a",
   "metadata": {},
   "source": [
    "analyse baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fco2models.utraining import prep_data\n",
    "val_df = pd.read_parquet('../data/training_data/valdf_100km_random_reshaped.pq')\n",
    "vald_df2021 = pd.read_parquet('../data/training_data/df_100km_random_reshaped_2021.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3129f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 19:50:49,816 - INFO - Filling missing sss_cci values with salt_soda values\n",
      "2025-04-15 19:50:49,851 - INFO - predictors: ['sst_cci', 'sss_cci', 'chl_globcolour', 'year', 'lon', 'lat']\n",
      "2025-04-15 19:50:49,977 - INFO - clipping fco2 values to 0-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after filtering:  22441\n",
      "(6, 22441, 64) (22441, 64)\n",
      "(6, 22441, 64) (1, 22441, 64)\n",
      "number of fco2 measurements greater than 500:  7189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 19:50:50,193 - INFO - Filling missing sss_cci values with salt_soda values\n",
      "2025-04-15 19:50:50,213 - INFO - predictors: ['sst_cci', 'sss_cci', 'chl_globcolour', 'year', 'lon', 'lat']\n",
      "2025-04-15 19:50:50,277 - INFO - clipping fco2 values to 0-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after filtering:  11327\n",
      "(6, 11327, 64) (11327, 64)\n",
      "(6, 11327, 64) (1, 11327, 64)\n",
      "number of fco2 measurements greater than 500:  17121\n",
      "val_ds_baseline shape:  (33768, 7, 64)\n"
     ]
    }
   ],
   "source": [
    "predictors = ['sst_cci', 'sss_cci', 'chl_globcolour', 'year', 'lon', 'lat']\n",
    "val_ds_baseline = prep_data(val_df, predictors)\n",
    "val_ds_2021_baseline = prep_data(vald_df2021, predictors)\n",
    "val_ds_baseline = np.concatenate((val_ds_baseline, val_ds_2021_baseline), axis=0)\n",
    "print(\"val_ds_baseline shape: \", val_ds_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce8bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f68b83cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# load baseline model\n",
    "save_path = '../models/baseline/'\n",
    "model_path = 'final_model_e_5.pt'\n",
    "model_class = UNet2DModelWrapper\n",
    "\n",
    "params = json.load(open(save_path + 'hyperparameters.json', 'r'))\n",
    "checkpoint = torch.load(save_path + model_path)\n",
    "model = UNet2DModelWrapper(**params['model_params'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16a45b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of predictor  0 :  1.0\n",
      "Min value of predictor  0 :  -0.9042751149016895\n",
      "Max value of predictor  1 :  0.9907780542065303\n",
      "Min value of predictor  1 :  -0.994580936660723\n",
      "Max value of predictor  2 :  0.9894330785391408\n",
      "Min value of predictor  2 :  -1.2438073092147035\n",
      "Max value of predictor  3 :  0.6568312421999736\n",
      "Min value of predictor  3 :  -0.9999623124466104\n",
      "Max value of predictor  4 :  1.0\n",
      "Min value of predictor  4 :  -1.0\n",
      "Max value of predictor  5 :  0.999994405110677\n",
      "Min value of predictor  5 :  -0.999987777777844\n",
      "Max value of predictor  6 :  0.9898566590539928\n",
      "Min value of predictor  6 :  -0.9961489627443083\n"
     ]
    }
   ],
   "source": [
    "# normalize the data\n",
    "train_maxs = params['train_maxs']\n",
    "train_mins = params['train_mins']\n",
    "\n",
    "for i in range(val_ds_baseline.shape[1]):\n",
    "    val_ds_baseline[:, i, :] = 2 * (val_ds_baseline[:, i, :] - train_mins[i]) / (train_maxs[i] - train_mins[i]) - 1\n",
    "    # print max and min values\n",
    "    print(\"Max value of predictor \", i, \": \", np.nanmax(val_ds_baseline[:, i, :]))\n",
    "    print(\"Min value of predictor \", i, \": \", np.nanmin(val_ds_baseline[:, i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7749fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def baseline_eval_loop(model, val_dataloader, device, random_model=None):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "    losses = []\n",
    "    preds = []\n",
    "    random_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = batch.to(device)\n",
    "            target = batch[:, 0:1, :]\n",
    "            context = batch[:, 1:, :]\n",
    "            nan_mask = torch.isnan(target)\n",
    "            # replace nan with zeros\n",
    "            target = torch.where(nan_mask, torch.zeros_like(target), target).float()\n",
    "            #concatenate the noisy target with the context and the mask\n",
    "            input = torch.cat([context, (~nan_mask).float()], dim=1)\n",
    "            input = input.to(device).float()\n",
    "            mean_pred = model(input, torch.zeros(batch.shape[0], ).to(device), return_dict=False)[0]\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(mean_pred[~nan_mask], target[~nan_mask])\n",
    "            #val_loss += loss\n",
    "            losses.append(loss.cpu().numpy())\n",
    "            preds.append(mean_pred.cpu().numpy())\n",
    "            if random_model is not None:\n",
    "                # calculate random model prediction\n",
    "                random_pred = random_model(input, torch.zeros(batch.shape[0], ).to(device), return_dict=False)[0]\n",
    "                random_preds.append(random_pred.cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    print(\"preds shape: \", preds.shape)\n",
    "    losses = np.concatenate(losses, axis=0)\n",
    "    print(\"losses shape: \", losses.shape)\n",
    "    random_preds = np.concatenate(random_preds, axis=0)\n",
    "    print(\"random_preds shape: \", random_preds.shape)\n",
    "    return preds, losses, random_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [00:29<00:00, 17.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds shape:  (33768, 1, 64)\n",
      "losses shape:  (1751727,)\n",
      "random_preds shape:  (33768, 1, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "val_dataloader = DataLoader(val_ds_baseline, batch_size=64, shuffle=False)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "preds = []\n",
    "losses = []\n",
    "\n",
    "random_model = UNet2DModelWrapper(**params['model_params'])\n",
    "random_model.to(device)\n",
    "random_model.eval()\n",
    "random_preds = []\n",
    "preds, losses, random_preds = baseline_eval_loop(model, val_dataloader, device, random_model=random_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a9d42f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.030027466)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16044c9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m158\u001b[39m\n\u001b[1;32m----> 7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mpreds\u001b[49m[ix, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_ds_baseline[ix, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(random_preds[ix, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom prediction\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# plot a prediction and the original data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "ix = 158\n",
    "plt.plot(preds[ix, 0, :].T, 'o-', label='prediction', color='blue', alpha=0.5)\n",
    "plt.plot(val_ds_baseline[ix, 0, :].T, 'o-', label='original', color='red', alpha=1, linewidth=2)\n",
    "plt.plot(random_preds[ix, 0, :].T, 'o-', label='random prediction', color='green', alpha=0.5)\n",
    "plt.title('Prediction vs original')\n",
    "plt.xlabel('bins')\n",
    "plt.ylabel('fCO2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# set figure size\n",
    "\n",
    "plt.gcf().set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c1d988bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33768, 64), (33768, 64))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:, 0, :].shape, val_ds_baseline[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0f5e9a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Temp\\ipykernel_7652\\2237865156.py:9: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr[i], _ = pearsonr(preds[i, :], targets[i, :])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation coefficient:  0.2158618129307957\n"
     ]
    }
   ],
   "source": [
    "# calculate average correlation coefficient\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def calculate_pearsonr(preds, targets):\n",
    "    preds = preds.reshape(preds.shape[0], -1)\n",
    "    targets = targets.reshape(targets.shape[0], -1)\n",
    "    corr = np.zeros(preds.shape[0])\n",
    "    for i in range(preds.shape[0]):\n",
    "        corr[i], _ = pearsonr(preds[i, :], targets[i, :])\n",
    "    return np.nanmean(corr)\n",
    "\n",
    "corr = calculate_pearsonr(preds, val_ds_baseline[:, 0, :])\n",
    "print(\"Average correlation coefficient: \", corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2ede766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Temp\\ipykernel_7652\\2237865156.py:9: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr[i], _ = pearsonr(preds[i, :], targets[i, :])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation coefficient of random model:  0.022956432073763855\n"
     ]
    }
   ],
   "source": [
    "random_corr = calculate_pearsonr(random_preds, val_ds_baseline[:, 0, :])\n",
    "print(\"Average correlation coefficient of random model: \", random_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "88463706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  24.511798163255413\n"
     ]
    }
   ],
   "source": [
    "scaled_preds =  (preds[:, 0, :] + 1) * (train_maxs[0] - train_mins[0]) / 2 + train_mins[0]\n",
    "scaled_true = (val_ds_baseline[:, 0, :] + 1) * (train_maxs[0] - train_mins[0]) / 2 + train_mins[0]\n",
    "\n",
    "# calculate RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmses = []\n",
    "for i in range(scaled_preds.shape[0]):\n",
    "    nan_mask = np.isnan(scaled_true[i, :])\n",
    "    rmses.append(np.sqrt(mean_squared_error(scaled_preds[i, :][~nan_mask], scaled_true[i, :][~nan_mask])))\n",
    "print(\"RMSE: \", np.mean(rmses))\n",
    "rmses = np.array(rmses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463401e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
