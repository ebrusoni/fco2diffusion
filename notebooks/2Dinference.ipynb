{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BrainSliceConditionalDiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a conditional diffusion model over 3D brain volumes by slicing\n",
    "    along multiple axes and processing with a difusion 2D U-Net. Optionally performs\n",
    "    inpainting where ground-truth measurements are available.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_shape, channels_indices, unet, buffer=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_shape (tuple): Shape of the input volume, e.g., (D, H, W).\n",
    "            channels_indices (list[int]): Which channels in the reference images to use.\n",
    "            unet (nn.Module): 2D U-Net model for slice-wise denoising.\n",
    "            buffer (int): Number of extra timesteps to pad at the start/end.\n",
    "        \"\"\"\n",
    "        super(BrainSliceConditionalDiffusion, self).__init__()\n",
    "        self.unet = unet                      # 2D denoiser network\n",
    "        self.buffer = buffer                  # padding around time range\n",
    "        self.channels = channels_indices      # channels used for conditioning\n",
    "\n",
    "    def inpainting(self, pred_x_0, integer_brain_coord, brain2_slices_rs, reference_image):\n",
    "        \"\"\"\n",
    "        Replace predicted voxels at known coordinates with ground-truth values.\n",
    "\n",
    "        Args:\n",
    "            pred_x_0 (Tensor): Current clean volume prediction, shape (D,H,W,C).\n",
    "            integer_brain_coord (LongTensor): Indices of known voxels, shape (N,3).\n",
    "            brain2_slices_rs (Tensor): Reference slices, shape (N, num_channels).\n",
    "            reference_image (Tensor): Full conditioning volume.\n",
    "        Returns:\n",
    "            Tensor: Updated pred_x_0 with inpainted values.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Zero out predicted values at known coords\n",
    "            pred_x_0[\n",
    "                integer_brain_coord[:, 0],\n",
    "                integer_brain_coord[:, 1],\n",
    "                integer_brain_coord[:, 2],\n",
    "                :\n",
    "            ] = 0\n",
    "            # Insert ground-truth slice values for specified channels\n",
    "            pred_x_0[\n",
    "                integer_brain_coord[:, 0],\n",
    "                integer_brain_coord[:, 1],\n",
    "                integer_brain_coord[:, 2],\n",
    "                :\n",
    "            ] += brain2_slices_rs[:, self.channels].clone().float()\n",
    "            # Alternative: inpaint only last channel with reference_image\n",
    "            # pred_x_0[:, :, :, -1] = reference_image\n",
    "        return pred_x_0\n",
    "\n",
    "    def predict_x_0(self, t, x_t, noise_scheduler):\n",
    "        \"\"\"\n",
    "        Use the U-Net to predict the denoised image x_0 from noisy input x_t.\n",
    "\n",
    "        Args:\n",
    "            t (int): Current timestep index.\n",
    "            x_t (Tensor): Noisy input at timestep t.\n",
    "            noise_scheduler: Scheduler to embed t.\n",
    "        Returns:\n",
    "            Tensor: Predicted clean image x_0 on CPU.\n",
    "        \"\"\"\n",
    "        # Select device (GPU if available)\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        timesteps = torch.LongTensor([t]).to(device)\n",
    "        # Forward pass through U-Net: returns (noise_pred, ...) or x0_pred\n",
    "        x_0_pred = self.unet(x_t.to(device), timesteps, return_dict=False)[0] # here I used the direct diffusion prediction of x_0\n",
    "        return x_0_pred.cpu()\n",
    "\n",
    "    def step(\n",
    "        self, i, t, iterator, pred_x_0,\n",
    "        noise_scheduler, num_steps,\n",
    "        integer_brain_coord, brain2_slices_rs, reference_image,\n",
    "        batch_size, verbose=True, with_inpainting=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform one diffusion step: add noise, slice-roll-slice, denoise on batches,\n",
    "        reassemble volume, and optionally inpaint known voxels.\n",
    "\n",
    "        Args:\n",
    "            i (int): Index of current step in loop.\n",
    "            t (int): Diffusion timestep.\n",
    "            iterator: tqdm iterator for progress updates.\n",
    "            pred_x_0 (Tensor): Current predicted clean volume.\n",
    "            noise_scheduler: Scheduler for noise addition/removal.\n",
    "            num_steps (int): Total number of denoising steps.\n",
    "            integer_brain_coord, brain2_slices_rs, reference_image: inpainting data.\n",
    "            batch_size (int): Number of slices per batch.\n",
    "            verbose (bool): Whether to update iterator.\n",
    "            with_inpainting (bool): Flag to apply inpainting.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Updated pred_x_0 after this step.\n",
    "        \"\"\"\n",
    "        # Ensure a device variable exists (define as in predict_x_0)\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        # 1. Sample noisy input x_t for this timestep\n",
    "        timestep = torch.LongTensor([t]).to(device)\n",
    "        noise = torch.randn_like(pred_x_0)\n",
    "        x_t = noise_scheduler.add_noise(pred_x_0, noise, timestep)\n",
    "\n",
    "        # 2. Define axis permutations to slice along different views\n",
    "        permutations = [\n",
    "            [0, 1, 2, 3],  # original volume\n",
    "            [1, 2, 0, 3],  # rotate axes: H-W-D\n",
    "            [2, 0, 1, 3],  # rotate axes: W-D-H\n",
    "        ]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Choose a permutation based on iteration\n",
    "            permutation = permutations[i % len(permutations)]\n",
    "            iterator.set_postfix({\"state\": \"rolling\"})\n",
    "\n",
    "            # Random spatial shift to vary slice boundaries\n",
    "            shifts = torch.randint(-16, 16, (3,))\n",
    "            rolled = x_t.roll(list(shifts), dims=[0, 1, 2])\n",
    "            rolled_ref = reference_image.clone().roll(list(shifts), dims=[0, 1, 2])\n",
    "\n",
    "            # 3. Flatten volume into batches of 2D slices\n",
    "            axial_batches = flatten(permutation, rolled)\n",
    "            axial_conditioning = flatten(permutation, rolled_ref.unsqueeze(-1))\n",
    "\n",
    "            # Preallocate prediction buffer\n",
    "            pred_buf = torch.zeros_like(axial_batches)\n",
    "            assert len(axial_batches) % batch_size == 0, \"Batch size must divide number of slices\"\n",
    "\n",
    "            # 4. Create minibatches: shape [n_batches, batch_size, C, H, W]\n",
    "            xb = axial_batches.unfold(0, batch_size, batch_size).permute(0, 3, 1, 2)\n",
    "            cb = axial_conditioning.unfold(0, batch_size, batch_size).permute(0, 3, 1, 2)\n",
    "            inputs = torch.cat([xb, cb], dim=2)\n",
    "\n",
    "            # 5. Denoise each minibatch\n",
    "            for j, batch in enumerate(inputs):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                pred_buf[start:end] = self.predict_x_0(t, batch, noise_scheduler)\n",
    "                iterator.set_postfix({\"state\": f\"running batch {j}\"})\n",
    "\n",
    "            # Make contiguous for reshaping\n",
    "            if not pred_buf.is_contiguous():\n",
    "                pred_buf = pred_buf.contiguous()\n",
    "\n",
    "            # 6. Unflatten back to 3D volume\n",
    "            iterator.set_postfix({\"state\": \"unflattening\"})\n",
    "            pred_vol = unflatten(permutation, pred_buf, x_t.shape)\n",
    "\n",
    "            # 7. Reverse the random shift\n",
    "            iterator.set_postfix({\"state\": \"unrolling\"})\n",
    "            inv_shifts = [-s.item() for s in shifts]\n",
    "            pred_vol = pred_vol.roll(inv_shifts, dims=[0, 1, 2])\n",
    "\n",
    "            # 8. Optional inpainting of known measurements\n",
    "            if with_inpainting:\n",
    "                pred_vol = self.inpainting(\n",
    "                    pred_vol, integer_brain_coord,\n",
    "                    brain2_slices_rs, reference_image\n",
    "                )\n",
    "\n",
    "            return pred_vol\n",
    "\n",
    "    def diffusion_pipeline(\n",
    "        self, x_0_start, t_start, t_end, noise_scheduler,\n",
    "        batch_size, integer_brain_coord, brain2_slices_rs,\n",
    "        reference_image, num_steps=50, verbose=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the full reverse diffusion from t_start down to t_end over num_steps.\n",
    "\n",
    "        Args:\n",
    "            x_0_start (Tensor): Initial guess for the volume.\n",
    "            t_start (int): Starting (noisiest) timestep.\n",
    "            t_end (int): Ending (cleanest) timestep.\n",
    "            noise_scheduler: Diffusion noise scheduler.\n",
    "            batch_size (int): Slices per batch in step().\n",
    "            integer_brain_coord, brain2_slices_rs, reference_image: inpainting data.\n",
    "            num_steps (int): Number of denoising iterations.\n",
    "            verbose (bool): If True, show tqdm progress.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Final denoised volume.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Copy the initial estimate\n",
    "            pred_x_0 = x_0_start.clone()\n",
    "\n",
    "            # Define a sequence of timesteps\n",
    "            timesteps = torch.linspace(t_start, t_end, num_steps + 1).int()\n",
    "            iterator = tqdm(range(1, len(timesteps)), disable=not verbose)\n",
    "\n",
    "            # Iterate through timesteps and refine prediction\n",
    "            for i in iterator:\n",
    "                t = timesteps[i - 1].item()\n",
    "                pred_x_0 = self.step(\n",
    "                    i, t, iterator, pred_x_0,\n",
    "                    noise_scheduler, num_steps,\n",
    "                    integer_brain_coord, brain2_slices_rs,\n",
    "                    reference_image, batch_size,\n",
    "                    verbose, with_inpainting=False\n",
    "                )\n",
    "            return pred_x_0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
